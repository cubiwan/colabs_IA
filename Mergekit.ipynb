{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMh2kx0URRO+f0FIYxzjMA8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cubiwan/colabs_IA/blob/main/Mergekit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Ejemplo de uso de [mergekit](https://github.com/cg123/mergekit)**"
      ],
      "metadata": {
        "id": "mygGwVQHz_HN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4x4Qs4sduPFg"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/cg123/mergekit.git\n",
        "%cd mergekit\n",
        "%pip install -e .\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ficheros de configuracion:\n"
      ],
      "metadata": {
        "id": "wWnyKXeZ0QM_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import yaml\n",
        "\n",
        "#passthrough - añadir capas\n",
        "yaml_config_passthrough = \"\"\"\n",
        "merge_method: passthrough\n",
        "dtype: bfloat16\n",
        "\n",
        "slices:\n",
        "  - sources:\n",
        "    - model: rhysjones/phi-2-orange\n",
        "      layer_range: [0, 32]\n",
        "  - sources:\n",
        "    - model: cognitivecomputations/dolphin-2_6-phi-2\n",
        "      layer_range: [0, 32]\n",
        "merge_method: passthrough\n",
        "dtype: bfloat16\n",
        "\"\"\"\n",
        "\n",
        "#linear - mezclar capas\n",
        "yaml_config_linear = \"\"\"\n",
        "models:\n",
        "  - model: rhysjones/phi-2-orange\n",
        "    parameters:\n",
        "      weight: 1.0\n",
        "  - model: cognitivecomputations/dolphin-2_6-phi-2\n",
        "    parameters:\n",
        "      weight: 0.8\n",
        "merge_method: linear\n",
        "dtype: float16\n",
        "\"\"\"\n",
        "\n"
      ],
      "metadata": {
        "id": "3GZoetHt3XTC"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Configuración"
      ],
      "metadata": {
        "id": "9gynwN0f1IeI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "OUTPUT_PATH = \"./merged\"  # folder to store the result in\n",
        "LORA_MERGE_CACHE = \"/tmp\"  # change if you want to keep these for some reason\n",
        "#CONFIG_YML = \"./examples/gradient-slerp.yml\"  # merge configuration file\n",
        "CONFIG_YML = \"./config.yml\"  # merge configuration file\n",
        "COPY_TOKENIZER = True  # you want a tokenizer? yeah, that's what i thought\n",
        "LAZY_UNPICKLE = True  # experimental low-memory model loader\n",
        "LOW_CPU_MEMORY = True  # enable if you somehow have more VRAM than RAM+swap\n",
        "OUT_SHARD_SIZE = \"1B\"\n",
        "yaml_config = yaml_config_linear #mezcla lineal\n",
        "#yaml_config = yaml_config_passthrough #frankenmerging\n",
        "\n",
        "# Save config as yaml file\n",
        "with open('./config.yml', 'w', encoding=\"utf-8\") as f:\n",
        "    f.write(yaml_config)"
      ],
      "metadata": {
        "id": "EREArnIpu1OI"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mezcla:"
      ],
      "metadata": {
        "id": "DY0NrBN41L9q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# actually do merge\n",
        "import torch\n",
        "\n",
        "\n",
        "from mergekit.config import MergeConfiguration\n",
        "from mergekit.merge import MergeOptions, run_merge\n",
        "from mergekit.common import parse_kmb\n",
        "\n",
        "\n",
        "with open(CONFIG_YML, \"r\", encoding=\"utf-8\") as fp:\n",
        "    merge_config = MergeConfiguration.model_validate(yaml.safe_load(fp))\n",
        "\n",
        "run_merge(\n",
        "    merge_config,\n",
        "    out_path=OUTPUT_PATH,\n",
        "    options=MergeOptions(\n",
        "        lora_merge_cache=LORA_MERGE_CACHE,\n",
        "        cuda=torch.cuda.is_available(),\n",
        "        copy_tokenizer=COPY_TOKENIZER,\n",
        "        lazy_unpickle=LAZY_UNPICKLE,\n",
        "        low_cpu_memory=LOW_CPU_MEMORY,\n",
        "        out_shard_size = parse_kmb(OUT_SHARD_SIZE),\n",
        "        trust_remote_code=True\n",
        "    )\n",
        ")\n",
        "print(\"Done!\")"
      ],
      "metadata": {
        "id": "2bv_kCmJu11A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Probar el modelo"
      ],
      "metadata": {
        "id": "cG1le0Bw1A2R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers bitsandbytes>=0.39.0 -q\n",
        "!pip install einops"
      ],
      "metadata": {
        "id": "gSQNukztPBBr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "model_path = \"./merged\"\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(model_path, device_map=\"auto\")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n"
      ],
      "metadata": {
        "id": "NxtgBXQAPGeu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#test\n",
        "model_inputs = tokenizer([\"A list of colors: red, blue\"], return_tensors=\"pt\").to(\"cuda\")\n",
        "generated_ids = model.generate(**model_inputs)\n",
        "tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]"
      ],
      "metadata": {
        "id": "fC-prM8zzx1c"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}